{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntax natürlicher Sprachen, WS 2023/24\n",
    "\n",
    "# 12 - Übung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /opt/homebrew/Cellar/gpgme/1.23.2/lib/python3.12/site-packages/gpg-1.23.2-py3.12-macosx-14-arm64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting svgling\n",
      "  Using cached svgling-0.4.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting svgwrite (from svgling)\n",
      "  Using cached svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
      "Using cached svgling-0.4.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: svgwrite, svgling\n",
      "Successfully installed svgling-0.4.0 svgwrite-1.4.3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import nltk\n",
    "from nltk import conlltags2tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1 - IOB-Tagging\n",
    "\n",
    "### a) Vervollständigen Sie den die folgende *IOB-Tag-Sequenz* einer NP-VP-Chunk-Analyse erfüllenden deutschen Satz.\n",
    "\n",
    "#### (Beachten Sie auch die gegebenen Satzzeichen bzgl. möglicher Satztypen und entsprechender Worstellung.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iob_list = [\n",
    "(\"B-NP\", \"\"),\n",
    "(\"B-VP\", \"\"),\n",
    "(\"O\", \"\"),\n",
    "(\"O\", \",\"),\n",
    "(\"O\", \"\"),\n",
    "(\"B-NP\", \"\"),\n",
    "(\"B-NP\", \"\"),\n",
    "(\"I-NP\", \"\"),\n",
    "(\"O\", \"\"),\n",
    "(\"B-VP\", \"\"),\n",
    "(\"O\", \".\")\n",
    "]\n",
    "\n",
    "tree = conlltags2tree([(word, \"\", iobtag)for iobtag, word in iob_list])\n",
    "tree.pretty_print(unicodelines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Geben Sie die *IOB-Tag-Sequenz* und *POS-Label* einer vollständigen flachen Chunk-Analyse für den folgenden Satz an:\n",
    "\n",
    "- *Während sich Kafka in Briefen, Tagebüchern und Prosatexten umfangreich mit seinem Verhältnis zum Vater auseinandersetzte, stand die Beziehung zu seiner Mutter eher im Hintergrund.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         S            \n",
      "         │             \n",
      "         NP           \n",
      "   ┌─────┴──────┐      \n",
      "ein/DET     Beispiel/N\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iob_list = [\n",
    "(\"ein\", \"DET\", \"B-NP\"),\n",
    "(\"Beispiel\", \"N\", \"I-NP\"),\n",
    "]\n",
    "\n",
    "tree = conlltags2tree(iob_list)\n",
    "tree.pretty_print(unicodelines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iob_list = [\n",
    "(\"Während\", \"\", \"\"),\n",
    "(\"sich\", \"\", \"B-NP\"),\n",
    "(\"Kafka\", \"\", \"B-NP\"),\n",
    "(\"in\", \"\", \"\"),\n",
    "(\"Briefen,\", \"\", \"B-NP\"),\n",
    "(\"Tagebüchern\", \"\", \"B-NP\"),\n",
    "(\"und\", \"\", \"\"),\n",
    "(\"Prosatexten\", \"\", \"B-NP\"),\n",
    "(\"umfangreich\", \"\", \"\"),\n",
    "(\"mit\", \"\", \"\"),\n",
    "(\"seinem\", \"\", \"\"),\n",
    "(\"schwierigen\", \"\", \"\"),\n",
    "(\"Verhältnis\", \"\", \"\"),\n",
    "(\"zum\", \"\", \"\"),\n",
    "(\"Vater\", \"\", \"\"),\n",
    "(\"auseinandersetzte\", \"\", \"\"),\n",
    "(\",\", \"PUNCT\", \"O\"),\n",
    "(\"stand\", \"\", \"\"),\n",
    "(\"die\", \"\", \"\"),\n",
    "(\"Beziehung\", \"\", \"\"),\n",
    "(\"zu\", \"\", \"\"),\n",
    "(\"seiner\", \"\", \"\"),\n",
    "(\"Mutter\", \"\", \"\"),\n",
    "(\"eher\", \"\", \"\"),\n",
    "(\"im\", \"\", \"\"),\n",
    "(\"Hintergrund\", \"\", \"\"),\n",
    "(\".\", \"PUNCT\", \"O\")\n",
    "]\n",
    "\n",
    "tree = conlltags2tree(iob_list)\n",
    "tree.pretty_print(unicodelines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Aufgabe 2 - Evaluationsmetriken*\n",
    "\n",
    "#### Betrachten Sie folgende Daten. Es handelt sich um ein vereinfachtes Tagging-Schema fürs Chunking, bei dem nur zwischen „Teil einer NP“ (`1`) und „nicht Teil einer NP“ (`0`) unterschieden wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = [1,0,1,0,0,1,1,1,1,0]\n",
    "chunker1     = [1,1,1,0,1,0,1,1,1,1]\n",
    "chunker2     = [1,0,1,0,0,0,0,0,1,0]\n",
    "chunker3     = [0,0,0,0,0,1,1,1,1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Berechnen Sie für jeden der Chunker Accuracy, Precision, Recall und F1-Score zunächst per Hand und überprüfen Sie dann Ihr Ergebnis mit dem folgenden Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(chunker):\n",
    "    print(\n",
    "        \"Accuracy:\",\n",
    "        \"{:.2f}\".format(accuracy_score(ground_truth, chunker))\n",
    "    )\n",
    "    print(\n",
    "        \"Precision:\",\n",
    "        \"{:.2f}\".format(precision_score(ground_truth, chunker))\n",
    "    )\n",
    "    print(\n",
    "        \"Recall:\",\n",
    "        \"{:.2f}\".format(recall_score(ground_truth, chunker))\n",
    "    )\n",
    "    print(\n",
    "        \"F1-Score:\",\n",
    "        \"{:.2f}\".format(f1_score(ground_truth, chunker))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunker 1\n",
    "\n",
    "#Accuracy: 6/10 = 0,6\n",
    "#Precision: 5/8 = 0,63\n",
    "#Recall: 5/6 = 0,83\n",
    "#F1: (2*0,63*0,83)/(0,63+0,83)=0,72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.60\n",
      "Precision: 0.62\n",
      "Recall: 0.83\n",
      "F1-Score: 0.71\n"
     ]
    }
   ],
   "source": [
    "accuracy(chunker1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunker 2\n",
    "\n",
    "#Accuracy: 7/10=0,7\n",
    "#Precision: 3/3=1\n",
    "#Recall: 3/6=0,5\n",
    "#F1: 1/1,5=0,67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.70\n",
      "Precision: 1.00\n",
      "Recall: 0.50\n",
      "F1-Score: 0.67\n"
     ]
    }
   ],
   "source": [
    "accuracy(chunker2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunker 3\n",
    "\n",
    "#Accuracy: 0,8\n",
    "#Precision: 4/4=1\n",
    "#Recall: 4/6= 0,67\n",
    "#F1: (2*0,67)/1,67=0,80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80\n",
      "Precision: 1.00\n",
      "Recall: 0.67\n",
      "F1-Score: 0.80\n"
     ]
    }
   ],
   "source": [
    "accuracy(chunker3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Aufgabe 3 - Herunterladen von Ressourcen*\n",
    "\n",
    "#### Das CoNLL 2000 Korpus ist ein POS- und Chunk-getaggtes Korpus (IOB- Format), das in ein Test- und ein Trainingskorpus aufgeteilt ist. Wir werden es zum Training und zur Evaluation von Chunk-Parsern verwenden. Laden Sie es sich dafür zunächst über die Ressource `corpora/conll2000` herunter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wenn Sie es erfolgreich heruntergeladen haben, können Sie folgendermaßen darauf zugreifen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,512.0,168.0\" width=\"512px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"9.375%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Over</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"4.6875%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"14.0625%\" x=\"9.375%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"44.4444%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">a</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"22.2222%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"55.5556%\" x=\"44.4444%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">cup</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"72.2222%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"16.4062%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.25%\" x=\"23.4375%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">of</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.5625%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"12.5%\" x=\"29.6875%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">coffee</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"35.9375%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"4.6875%\" x=\"42.1875%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"44.5312%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"18.75%\" x=\"46.875%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"41.6667%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Mr.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"20.8333%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"58.3333%\" x=\"41.6667%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Stone</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"70.8333%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"56.25%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"9.375%\" x=\"65.625%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">told</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"70.3125%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"20.3125%\" x=\"75%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"46.1538%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">his</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP$</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"23.0769%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"53.8462%\" x=\"46.1538%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">story</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"73.0769%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"85.1562%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"4.6875%\" x=\"95.3125%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"97.6562%\" y1=\"19.2px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [('Over', 'IN'), Tree('NP', [('a', 'DT'), ('cup', 'NN')]), ('of', 'IN'), Tree('NP', [('coffee', 'NN')]), (',', ','), Tree('NP', [('Mr.', 'NNP'), ('Stone', 'NNP')]), ('told', 'VBD'), Tree('NP', [('his', 'PRP$'), ('story', 'NN')]), ('.', '.')])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "conll2000.chunked_sents('train.txt', chunk_types=['NP'])[99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Das `chunk_types`-Argument dient der Auwahl von Chunk-Typen (in diesem Beispiel Nominalphrasen)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Aufgabe 4 - Chunking mit regulären Ausdrücken*\n",
    "\n",
    "#### Erstellen Sie einen einfachen `RegexpParser`, der für Nominalphrasen charakteristische Tags zu NPs zusammenfasst. Solche charakteristischen Tags sind z.B. Kardinalzahlen (`CD`), Artikel (`DT`), Adjektive (`JJ`, `JJR`, `JJS`) und natürlich Substantive (`NN`, `NNS`, `NNP`, `NNPS`).\n",
    "\n",
    "#### Weitere interessante Tags wären `PDT` (z.B. *both*, *a lot of*), `POS` (*'s*), `PRP` (Personalpronomen), `PRP$`(Possessivpronomen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r\"\"\"NP:\n",
    "{<>}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluieren Sie Ihren Parser anschließend auf dem CoNLL 2000 Korpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'regex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_sents \u001b[38;5;241m=\u001b[39m conll2000\u001b[38;5;241m.\u001b[39mchunked_sents(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, chunk_types\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNP\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m cp \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mRegexpParser(\u001b[43mregex\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(cp\u001b[38;5;241m.\u001b[39maccuracy(test_sents))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'regex' is not defined"
     ]
    }
   ],
   "source": [
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "cp = nltk.RegexpParser(regex)\n",
    "print(cp.accuracy(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Aufgabe 5 - Datenbasiertes Chunking*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Betrachten Sie den folgenden Code für einen Chunker, der für jedes POS-Tag das wahrscheinlichste Chunk-Tag berechnet (Training) und dieses dann zur Testzeit ausgibt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [\n",
    "            [\n",
    "                (t,c)\n",
    "                for w,t,c in nltk.chunk.tree2conlltags(sent)\n",
    "            ]\n",
    "            for sent in train_sents\n",
    "        ]\n",
    "        self.tagger = nltk.UnigramTagger(train_data)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [\n",
    "            chunktag for (pos, chunktag) in tagged_pos_tags\n",
    "        ]\n",
    "        conlltags = [\n",
    "            (word, pos, chunktag)\n",
    "            for ((word, pos), chunktag)\n",
    "            in zip(sentence, chunktags)\n",
    "        ]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainieren und evaluieren Sie den UnigramChunker auf dem CoNLL 2000 Korpus für NPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.9%%\n",
      "    Precision:     79.9%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     83.2%%\n"
     ]
    }
   ],
   "source": [
    "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
    "uc = UnigramChunker(train_sents)\n",
    "print(uc.accuracy(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Um uns einen Überblick darüber zu verschaffen, was der Chunker gelernt hat, können wir ihn für jedes mögliche POS-Tag eine Vorhersage treffen lassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 'B-NP'),\n",
       " ('$', 'B-NP'),\n",
       " (\"''\", 'O'),\n",
       " ('(', 'O'),\n",
       " (')', 'O'),\n",
       " (',', 'O'),\n",
       " ('.', 'O'),\n",
       " (':', 'O'),\n",
       " ('CC', 'O'),\n",
       " ('CD', 'I-NP'),\n",
       " ('DT', 'B-NP'),\n",
       " ('EX', 'B-NP'),\n",
       " ('FW', 'I-NP'),\n",
       " ('IN', 'O'),\n",
       " ('JJ', 'I-NP'),\n",
       " ('JJR', 'B-NP'),\n",
       " ('JJS', 'I-NP'),\n",
       " ('MD', 'O'),\n",
       " ('NN', 'I-NP'),\n",
       " ('NNP', 'I-NP'),\n",
       " ('NNPS', 'I-NP'),\n",
       " ('NNS', 'I-NP'),\n",
       " ('PDT', 'B-NP'),\n",
       " ('POS', 'B-NP'),\n",
       " ('PRP', 'B-NP'),\n",
       " ('PRP$', 'B-NP'),\n",
       " ('RB', 'O'),\n",
       " ('RBR', 'O'),\n",
       " ('RBS', 'B-NP'),\n",
       " ('RP', 'O'),\n",
       " ('SYM', 'O'),\n",
       " ('TO', 'O'),\n",
       " ('UH', 'O'),\n",
       " ('VB', 'O'),\n",
       " ('VBD', 'O'),\n",
       " ('VBG', 'O'),\n",
       " ('VBN', 'O'),\n",
       " ('VBP', 'O'),\n",
       " ('VBZ', 'O'),\n",
       " ('WDT', 'B-NP'),\n",
       " ('WP', 'B-NP'),\n",
       " ('WP$', 'B-NP'),\n",
       " ('WRB', 'O'),\n",
       " ('``', 'O')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postags = sorted(set(pos for sent in train_sents for (word,pos) in sent.leaves()))\n",
    "uc.tagger.tag(postags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) der `ConsecutiveNPChunker`, dessen Code Sie in der nächsten Zelle sehen, basiert auf einem Klassifikator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutiveNPChunkTagger(nltk.TaggerI):\n",
    "\n",
    "    def __init__(self, train_sents, npchunk_features):\n",
    "        self.extract_features = npchunk_features\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = npchunk_features(untagged_sent, i, history)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = self.extract_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n",
    "\n",
    "class ConsecutiveNPChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents, npchunk_features):\n",
    "        tagged_sents = [[((w,t),c) for (w,t,c) in\n",
    "                         nltk.chunk.tree2conlltags(sent)]\n",
    "                        for sent in train_sents]\n",
    "        self.tagger = ConsecutiveNPChunkTagger(tagged_sents, npchunk_features)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        tagged_sents = self.tagger.tag(sentence)\n",
    "        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dies erlaubt uns, die Features, die für die Klassifikation extrahiert werden, genauer zu bestimmen.\n",
    "\n",
    "#### Ein Feature-Extraktor lässt sich als Funktion z.B. so definieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_feature(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    return {\"pos\": pos}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dieser Feature-Extraktor extrahiert genau ein Feature, nämlich das POS-Tag, das auch der UnigramChunker verwendet hat.\n",
    "\n",
    "#### Evaluieren Sie den `ConsecutiveNPChunker` mit diesem Feature-Extraktor und vergleichen Sie seine Performanz mit der des `UnigramChunker`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.9%%\n",
      "    Precision:     79.9%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     83.2%%\n"
     ]
    }
   ],
   "source": [
    "chunker = ConsecutiveNPChunker(train_sents, pos_feature)\n",
    "print(chunker.accuracy(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Fügen Sie weitere Features für:\n",
    "- das aktuelle Wort\n",
    "- das vorhergehende POS-Tag\n",
    "- das vorhergehende Chunk-Tag\n",
    "\n",
    "#### zur Extraktion hinzu und beobachten Sie jeweils die Auswirkungen auf die Performanz in der Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_feature(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    return {\"pos\": pos}\n",
    "\n",
    "def previous_pos(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    return {\"pos\": pos}\n",
    "\n",
    "def previous_chunk(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    return {\"pos\": pos}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = ConsecutiveNPChunker(train_sents, word_feature)\n",
    "print(chunker.accuracy(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = ConsecutiveNPChunker(train_sents, previous_pos)\n",
    "print(chunker.accuracy(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = ConsecutiveNPChunker(train_sents, previous_chunk)\n",
    "print(chunker.accuracy(test_sents))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
